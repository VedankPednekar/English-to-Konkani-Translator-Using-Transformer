{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model from: /mnt/c/Users/Sohum/Documents/Transformer/TRANS_BASE_EK/saved_keras\n",
      "Model loaded successfully!\n",
      "\n",
      "Translating example sentences:\n",
      "Error translating 'How are you?': {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [1,1] vs. shape[1] = [1,1,1] [Op:ConcatV2] name: concat\n",
      "\n",
      "Error translating 'What is your name?': {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [1,1] vs. shape[1] = [1,1,1] [Op:ConcatV2] name: concat\n",
      "\n",
      "Error translating 'I am happy today.': {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [1,1] vs. shape[1] = [1,1,1] [Op:ConcatV2] name: concat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "MODEL_PATH = \"/mnt/c/Users/Vedank/Documents/Transformer/TRANS_BASE_EK/saved_keras\"\n",
    "CONTEXT_VOCAB_PATH = \"/mnt/c/Users/Vedank/Documents/Transformer/column1.txt.vocab\"\n",
    "TARGET_VOCAB_PATH = \"/mnt/c/Users/Vedank/Documents/Transformer/column2.txt.vocab\"\n",
    "\n",
    "# Check files\n",
    "if not os.path.exists(CONTEXT_VOCAB_PATH):\n",
    "    raise FileNotFoundError(f\"Context vocabulary file not found at {CONTEXT_VOCAB_PATH}\")\n",
    "if not os.path.exists(TARGET_VOCAB_PATH):\n",
    "    raise FileNotFoundError(f\"Target vocabulary file not found at {TARGET_VOCAB_PATH}\")\n",
    "\n",
    "# Load tokenizers\n",
    "context_tokenizer = tf_text.BertTokenizer(CONTEXT_VOCAB_PATH, lower_case=True)\n",
    "target_tokenizer = tf_text.BertTokenizer(TARGET_VOCAB_PATH, lower_case=True)\n",
    "\n",
    "# Load the SavedModel\n",
    "print(\"Loading the model from:\", MODEL_PATH)\n",
    "model = tf.saved_model.load(MODEL_PATH)\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_context(text, max_length=32):\n",
    "    tokens = context_tokenizer.tokenize(text).merge_dims(-2, -1)\n",
    "    tokens = tokens.to_tensor(default_value=0, shape=[1, max_length])\n",
    "    return tf.cast(tokens, tf.float32)  # Match signature\n",
    "\n",
    "# Define autoregressive translation\n",
    "def translate_text(input_text, max_length=32):\n",
    "    context_tokens = tokenize_context(input_text, max_length)\n",
    "    target_tokens = tf.cast(target_tokenizer.tokenize(\"<start>\").merge_dims(-2, -1).to_tensor(shape=[1, 1]), tf.float32)\n",
    "    output_ids = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Pad target_tokens to max_length\n",
    "        padded_target = tf.pad(target_tokens, [[0, 0], [0, max_length - tf.shape(target_tokens)[1]]], constant_values=0)\n",
    "        output_dict = model.signatures['serve'](args_0=context_tokens, args_0_1=padded_target)\n",
    "        logits = output_dict['output_0']  # Shape: (1, 32, vocab_size)\n",
    "        \n",
    "        # Get the next token from the current position\n",
    "        next_token_logits = logits[:, tf.shape(target_tokens)[1] - 1, :]\n",
    "        next_token = tf.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "        \n",
    "        if next_token.numpy()[0] == 3:  # Assuming 3 is <end> token ID\n",
    "            break\n",
    "        \n",
    "        output_ids.append(next_token.numpy()[0])\n",
    "        # Reshape next_token to (1, 1) and concatenate\n",
    "        next_token_2d = tf.cast(next_token[None], tf.float32)  # Shape: (1, 1)\n",
    "        target_tokens = tf.concat([target_tokens, next_token_2d], axis=1)\n",
    "    \n",
    "    # Detokenize the generated sequence\n",
    "    output_text = target_tokenizer.detokenize(tf.constant(output_ids, dtype=tf.int32))\n",
    "    return output_text.numpy().tobytes().decode(\"utf-8\")\n",
    "\n",
    "# Test the model\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    test_sentences = [\n",
    "        \"How are you?\",\n",
    "        \"What is your name?\",\n",
    "        \"I am happy today.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTranslating example sentences:\")\n",
    "    for sentence in test_sentences:\n",
    "        try:\n",
    "            translation = translate_text(sentence)\n",
    "            print(f\"Input: {sentence}\")\n",
    "            print(f\"Output: {translation}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating '{sentence}': {e}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
